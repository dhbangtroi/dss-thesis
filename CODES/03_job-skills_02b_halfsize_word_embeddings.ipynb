{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to work cross-platform\n",
    "import os\n",
    "\n",
    "# Libraries to work with dataset\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "\n",
    "# Library to reduce dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Libraries for monitoring operation process\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurate and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_name = os.name\n",
    "\n",
    "if os_name == 'nt':\n",
    "    \"\"\"Windows platform\"\"\"\n",
    "    BASE_DIR = \"E:/THIENDHB_GOOGLEDRIVE/MASTER TILBURG/THESIS/\"\n",
    "    INPUT_DIR = BASE_DIR + \"DATASET/INPUT/\"\n",
    "    OUTPUT_DIR = BASE_DIR + \"DATASET/OUTPUT/\"\n",
    "elif os_name == 'posix':\n",
    "    \"\"\"Linux platform\"\"\"\n",
    "    BASE_DIR = \"/media/pinkalinux/WORK/THIENDHB_GOOGLEDRIVE/MASTER TILBURG/THESIS/\"\n",
    "    INPUT_DIR = BASE_DIR + \"DATASET/INPUT/\"\n",
    "    OUTPUT_DIR = BASE_DIR + \"DATASET/OUTPUT/\"\n",
    "\n",
    "SEED = 6886\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gensim keyed vectors of word embeddings\n",
    "kv = KeyedVectors.load(OUTPUT_DIR + 'skill_word_norm_vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract element from keyed vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7949,\n",
       " ['anual',\n",
       "  'mp',\n",
       "  'word',\n",
       "  'librarys',\n",
       "  'outpatient',\n",
       "  'secretariat',\n",
       "  'structural',\n",
       "  'double',\n",
       "  'assignment',\n",
       "  'channel'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract vocabulary (words)\n",
    "X_vocab = kv.index_to_key\n",
    "len(X_vocab), X_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 7949/7949 [00:00<00:00, 441181.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7949, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract vectors (word embeddings)\n",
    "X_vectors = []\n",
    "for token in tqdm(X_vocab):\n",
    "    X_vectors.append(kv[token])\n",
    "X_vectors = np.asarray(X_vectors)\n",
    "X_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# PCA to get Top Components\n",
    "vector_size = X_vectors.shape[1]\n",
    "print(vector_size)\n",
    "pca_embeddings = {}\n",
    "\n",
    "pca = PCA(n_components=vector_size, random_state=SEED)\n",
    "X_vectors = X_vectors - np.mean(X_vectors)\n",
    "X_fit = pca.fit_transform(X_vectors)\n",
    "U1 = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7949it [00:00, 29659.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Post-Processing: Removing Projections on Top Components\n",
    "z = []\n",
    "\n",
    "for i, x in tqdm(enumerate(X_vectors)):\n",
    "    for u in U1[0:7]:\n",
    "        x = x - np.dot(u.transpose(), x) * u\n",
    "    z.append(x)\n",
    "\n",
    "z = np.asarray(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Dim Reduction\n",
    "pca = PCA(n_components=int(vector_size / 2), random_state=SEED)\n",
    "X_vectors = z - np.mean(z)\n",
    "X_new_final = pca.fit_transform(X_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to do Post-Processing Again\n",
    "pca = PCA(n_components=int(vector_size / 2), random_state=SEED)\n",
    "X_new = X_new_final - np.mean(X_new_final)\n",
    "X_new = pca.fit_transform(X_new)\n",
    "Ufit = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Processing: Removing Projections on Top Components again\n",
    "X_new_final = X_new_final - np.mean(X_new_final)\n",
    "\n",
    "final_pca_embeddings = []\n",
    "\n",
    "for i, x in enumerate(X_vocab):\n",
    "    final_pca_embeddings.append(X_new_final[i])\n",
    "\n",
    "    for u in Ufit[0:7]:\n",
    "        final_pca_embeddings[i] = (\n",
    "            final_pca_embeddings[i] - np.dot(u.transpose(), final_pca_embeddings[i]) * u\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "halfsize_kv = KeyedVectors(vector_size=int(vector_size / 2))\n",
    "halfsize_kv.add_vectors(keys=X_vocab, weights=final_pca_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Gensim keyed vectors to file\n",
    "halfsize_kv.save(OUTPUT_DIR + 'skill_halfsize_word_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize vectors\n",
    "halfsize_kv.fill_norms(force=True)\n",
    "\n",
    "# Save normalized keyed vectors to file\n",
    "halfsize_kv.save(OUTPUT_DIR + 'skill_halfsize_word_norm_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
