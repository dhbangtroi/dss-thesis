
@article{alelyaniFeatureSelectionClustering2013,
  title      = {Feature Selection for Clustering: {{A Review}}},
  shorttitle = {Feature Selection for Clustering},
  author     = {Alelyani, Salem and Tang, Jiliang and Liu, Huan},
  year       = {2013},
  volume     = {29},
  pages      = {144},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\FEATURE SELECTION\\Alelyani et al - Feature Selection for Clustering - A Review.pdf},
  journal    = {Data clustering: algorithms and applications},
  number     = {110-121}
}

@article{badihAssessingVariableImportance2019,
  title      = {Assessing Variable Importance in Clustering: A New Method Based on Unsupervised Binary Decision Trees},
  shorttitle = {Assessing Variable Importance in Clustering},
  author     = {Badih, Ghattas and Pierre, Michel and Laurent, Boyer},
  year       = {2019},
  month      = mar,
  volume     = {34},
  pages      = {301--321},
  issn       = {0943-4062, 1613-9658},
  doi        = {10.1007/s00180-018-0857-0},
  abstract   = {We consider different approaches for assessing variable importance in clustering. We focus on clustering using binary decision trees (CUBT), which is a non-parametric top-down hierarchical clustering method designed for both continuous and nominal data. We suggest a measure of variable importance for this method similar to the one used in Breiman's classification and regression trees. This score is useful to rank the variables in a dataset, to determine which variables are the most important or to detect the irrelevant ones. We analyze both stability and efficiency of this score on different data simulation models in the presence of noise, and compare it to other classical variable importance measures. Our experiments show that variable importance based on CUBT is much more efficient than other approaches in a large variety of situations.},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\FEATURE SELECTION\\Badih et al - 2019 - Assessing variable importance in clustering - a new method based on unsupervised.pdf},
  journal    = {Computational Statistics},
  language   = {en},
  number     = {1}
}

@article{blei2003latent,
  title     = {Latent dirichlet allocation},
  author    = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal   = {the Journal of machine Learning research},
  volume    = {3},
  pages     = {993--1022},
  year      = {2003},
  publisher = {JMLR. org}
}

@article{boselliClassifyingOnlineJob2018,
  title    = {Classifying Online {{Job Advertisements}} through {{Machine Learning}}},
  author   = {Boselli, Roberto and Cesarini, Mirko and Mercorio, Fabio and Mezzanzanica, Mario},
  year     = {2018},
  volume   = {86},
  pages    = {319--328},
  issn     = {0167739X},
  doi      = {10.1016/j.future.2018.03.035},
  file     = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Boselli et al - 2018 - Classifying online Job Advertisements through Machine Learning.pdf},
  journal  = {Future Generation Computer Systems},
  language = {en}
}

@article{breimanRandomForests2001,
  title     = {Random Forests},
  author    = {Breiman, Leo},
  year      = {2001},
  volume    = {45},
  pages     = {5--32},
  publisher = {{Springer}},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Breiman - 2001 - Random forests.pdf},
  journal   = {Machine learning},
  number    = {1}
}

@article{calancaResponsibleTeamPlayers2019,
  title      = {Responsible Team Players Wanted: An Analysis of Soft Skill Requirements in Job Advertisements},
  shorttitle = {Responsible Team Players Wanted},
  author     = {Calanca, Federica and Sayfullina, Luiza and Minkus, Lara and Wagner, Claudia and Malmi, Eric},
  year       = {2019},
  volume     = {8},
  pages      = {13},
  issn       = {2193-1127},
  doi        = {10.1140/epjds/s13688-019-0190-z},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Calanca et al - 2019 - Responsible team players wanted - an analysis of soft skill requirements in job.pdf},
  journal    = {EPJ Data Science},
  language   = {en},
  number     = {1}
}

@inproceedings{campelloDensitybasedClusteringBased2013,
  title     = {Density-Based Clustering Based on Hierarchical Density Estimates},
  booktitle = {Pacific-{{Asia}} Conference on Knowledge Discovery and Data Mining},
  author    = {Campello, Ricardo JGB and Moulavi, Davoud and Sander, J{\"o}rg},
  year      = {2013},
  pages     = {160--172},
  publisher = {{Springer}},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Campello et al - 2013 - Density-based clustering based on hierarchical density estimates2.pdf;C\:\\Users\\PinkaVivo\\Zotero\\storage\\6REKNH2H\\978-3-642-37456-2_14.html}
}

@inproceedings{daveCombinedRepresentationLearning2018,
  title     = {A {{Combined Representation Learning Approach}} for {{Better Job}} and {{Skill Recommendation}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author    = {Dave, Vachik S. and Zhang, Baichuan and Al Hasan, Mohammad and AlJadda, Khalifeh and Korayem, Mohammed},
  year      = {2018},
  pages     = {1997--2005},
  publisher = {{ACM}},
  address   = {{Torino Italy}},
  doi       = {10.1145/3269206.3272023},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Dave et al - 2018 - A Combined Representation Learning Approach for Better Job and Skill.pdf},
  isbn      = {978-1-4503-6014-2},
  language  = {en}
}

@article{demauroHumanResourcesBig2018,
  title      = {Human Resources for {{Big Data}} Professions: {{A}} Systematic Classification of Job Roles and Required Skill Sets},
  shorttitle = {Human Resources for {{Big Data}} Professions},
  author     = {De Mauro, Andrea and Greco, Marco and Grimaldi, Michele and Ritala, Paavo},
  year       = {2018},
  volume     = {54},
  pages      = {807--817},
  issn       = {03064573},
  doi        = {10.1016/j.ipm.2017.05.004},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\De Mauro et al - 2018 - Human resources for Big Data professions - A systematic classification of job.pdf},
  journal    = {Information Processing \& Management},
  language   = {en},
  number     = {5}
}

@article{djumalievaClassifyingOccupationsAccording2018,
  title   = {Classifying Occupations According to Their Skill Requirements in Job Advertisements},
  author  = {Djumalieva, Jyldyz and Lima, Antonio and Sleeman, Cath},
  year    = {2018},
  volume  = {4},
  pages   = {2018},
  file    = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Djumalieva et al - 2018 - Classifying occupations according to their skill requirements in job.pdf},
  journal = {Economic Statistics Centre of Excellence Discussion Paper}
}

@article{elghazelUnsupervisedFeatureSelection2015,
  title    = {Unsupervised Feature Selection with Ensemble Learning},
  author   = {Elghazel, Haytham and Aussem, Alex},
  year     = {2015},
  month    = jan,
  volume   = {98},
  pages    = {157--180},
  issn     = {1573-0565},
  doi      = {10.1007/s10994-013-5337-8},
  abstract = {In this paper, we show that the way internal estimates are used to measure variable importance in Random Forests are also applicable to feature selection in unsupervised learning. We propose a new method called Random Cluster Ensemble (RCE for short), that estimates the out-of-bag feature importance from an ensemble of partitions. Each partition is constructed using a different bootstrap sample and a random subset of the features. We provide empirical results on nineteen benchmark data sets indicating that RCE, boosted with a recursive feature elimination scheme (RFE) (Guyon and Elisseeff, Journal of Machine Learning Research, 3:1157\textendash 1182, 2003), can lead to significant improvement in terms of clustering accuracy, over several state-of-the-art supervised and unsupervised algorithms, with a very limited subset of features. The method shows promise to deal with very large domains. All results, datasets and algorithms are available on line (http://perso.univ-lyon1.fr/haytham.elghazel/RCE.zip).},
  file     = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Elghazel_Aussem - 2015 - Unsupervised feature selection with ensemble learning.pdf},
  journal  = {Machine Learning},
  language = {en},
  number   = {1}
}

@article{gagolewskiGenieNewFast2016a,
  title      = {Genie: {{A}} New, Fast, and Outlier-Resistant Hierarchical~Clustering~Algorithm},
  shorttitle = {Genie},
  author     = {Gagolewski, Marek and Bartoszuk, Maciej and Cena, Anna},
  year       = {2016},
  month      = oct,
  volume     = {363},
  pages      = {8--23},
  issn       = {0020-0255},
  doi        = {10.1016/j.ins.2016.05.003},
  abstract   = {The time needed to apply a hierarchical clustering algorithm is most often dominated by the number of computations of a pairwise dissimilarity measure. Such a constraint, for larger data sets, puts at a disadvantage the use of all the classical linkage criteria but the single linkage one. However, it is known that the single linkage clustering algorithm is very sensitive to outliers, produces highly skewed dendrograms, and therefore usually does not reflect the true underlying data structure \textendash{} unless the clusters are well-separated. To overcome its limitations, we propose a new hierarchical clustering linkage criterion called Genie. Namely, our algorithm links two clusters in such a way that a chosen economic inequity measure (e.g., the Gini- or Bonferroni-index) of the cluster sizes does not increase drastically above a given threshold. The presented benchmarks indicate a high practical usefulness of the introduced method: it most often outperforms the Ward or average linkage in terms of the clustering quality while retaining the single linkage speed. The Genie algorithm is easily parallelizable and thus may be run on multiple threads to speed up its execution further on. Its memory overhead is small: there is no need to precompute the complete distance matrix to perform the computations in order to obtain a desired clustering. It can be applied on arbitrary spaces equipped with a dissimilarity measure, e.g., on real vectors, DNA or protein sequences, images, rankings, informetric data, etc. A reference implementation of the algorithm has been included in the open source genie package for~R.},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Gagolewski et al - 2016 - Genie - A new, fast, and outlier-resistant hierarchical clustering algorithm.pdf},
  journal    = {Information Sciences},
  keywords   = {Gini-index,Hierarchical clustering,Inequity measures,Single linkage},
  language   = {en}
}

@inproceedings{griraUnsupervisedSemisupervisedClustering2005,
  title      = {Unsupervised and {{Semi}}-Supervised {{Clustering}}: A {{Brief Survey}}},
  shorttitle = {Unsupervised and {{Semi}}-Supervised {{Clustering}}},
  booktitle  = {In `{{A Review}} of {{Machine Learning Techniques}} for {{Processing Multimedia Content}}', {{Report}} of the {{MUSCLE European Network}} of {{Excellence}} ({{FP6}}},
  author     = {Grira, Nizar and Crucianu, Michel and Boujemaa, Nozha},
  year       = {2005},
  abstract   = {ia that provide significant distinctions between clustering methods and can help selecting appropriate candidate methods for one's problem:  . Objective of clustering. Many methods aim at finding a single partition of the collection of items into clusters. However, obtaining a hierarchy of clusters can provide more flexibility and other methods rather focus on this. A partition of the data can be obtained from a hierarchy by cutting the tree of clusters at some level.  . Nature of the data items. Most clustering methods were developed for numerical data, but some can deal with categorical data or with both numerical and categorical data.  . Nature of the available information. Many methods rely on rich representations of the data (e.g. vectorial) that let one define prototypes, data distributions, multidimensional intervals, etc., beside computing (dis)similarities. Other methods only require the evaluation of pairwise (dis)similarities between data items; while imposing less restricti},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Grira et al - 2005 - Unsupervised and Semi-supervised Clustering - a Brief Survey.pdf}
}

@book{hanDataMiningConcepts2012,
  title      = {Data Mining: Concepts and Techniques},
  shorttitle = {Data Mining},
  author     = {Han, Jiawei and Kamber, Micheline},
  year       = {2012},
  edition    = {3rd ed},
  publisher  = {{Elsevier}},
  address    = {{Burlington, MA}},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Han_Kamber - 2012 - Data mining - concepts and techniques.pdf},
  isbn       = {978-0-12-381479-1},
  keywords   = {Data mining},
  lccn       = {QA76.9.D343 H36 2012}
}

@inproceedings{liuComparativeStudyUnsupervised2005,
  title     = {A Comparative Study on Unsupervised Feature Selection Methods for Text Clustering},
  booktitle = {2005 {{International Conference}} on {{Natural Language Processing}} and {{Knowledge Engineering}}},
  author    = {Liu, Luying and Kang, Jianchu and Yu, Jing and Wang, Zhongliang},
  year      = {2005},
  month     = oct,
  pages     = {597--601},
  doi       = {10.1109/NLPKE.2005.1598807},
  abstract  = {Text clustering is one of the central problems in text mining and information retrieval area. For the high dimensionality of feature space and the inherent data sparsity, performance of clustering algorithms will dramatically decline. Two techniques are used to deal with this problem: feature extraction and feature selection. Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, four unsupervised feature selection methods, DF, TC, TVQ, and a new proposed method TV are introduced. Experiments are taken to show that feature selection methods can improves efficiency as well as accuracy of text clustering. Three clustering validity criterions are studied and used to evaluate clustering results.},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\FEATURE SELECTION\\Liu et al - 2005 - A comparative study on unsupervised feature selection methods for text.pdf},
  keywords  = {Clustering algorithms,Computer science,Feature extraction,Frequency,Information retrieval,Navigation,Principal component analysis,Text categorization,Text mining,TV}
}

@article{liuEvaluationFeatureSelection2003,
  title    = {An {{Evaluation}} on {{Feature Selection}} for {{Text Clustering}}},
  author   = {Liu, Tao and Liu, Shengping and Chen, Zheng and Ma, Wei-Ying},
  year     = {2003},
  pages    = {8},
  abstract = {Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, we first give empirical evidence that feature selection methods can improve the efficiency and performance of text clustering algorithm. Then we propose a new feature selection method called ``Term Contribution (TC)'' and perform a comparative study on a variety of feature selection methods for text clustering, including Document Frequency (DF), Term Strength (TS), Entropy-based (En), Information Gain (IG) and א2 statistic (CHI). Finally, we propose an ``Iterative Feature Selection (IF)'' method that addresses the unavailability of label problem by utilizing effective supervised feature selection method to iteratively select features and perform clustering. Detailed experimental results on Web Directory data are provided in the paper.},
  file     = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\FEATURE SELECTION\\Liu et al - An Evaluation on Feature Selection for Text Clustering.pdf},
  language = {en}
}

@article{liuIntegratingFeatureSelection2005,
  title     = {Toward Integrating Feature Selection Algorithms for Classification and Clustering},
  author    = {Liu, Huan and Yu, Lei},
  year      = {2005},
  volume    = {17},
  pages     = {491--502},
  publisher = {{IEEE}},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Liu_Yu - 2005 - Toward integrating feature selection algorithms for classification and.pdf},
  journal   = {IEEE Transactions on knowledge and data engineering},
  number    = {4}
}

@article{luRecommenderSystemApplication2015,
  title      = {Recommender System Application Developments: {{A}} Survey},
  shorttitle = {Recommender System Application Developments},
  author     = {Lu, Jie and Wu, Dianshuang and Mao, Mingsong and Wang, Wei and Zhang, Guangquan},
  year       = {2015},
  month      = jun,
  volume     = {74},
  pages      = {12--32},
  issn       = {0167-9236},
  doi        = {10.1016/j.dss.2015.03.008},
  abstract   = {A recommender system aims to provide users with personalized online product or service recommendations to handle the increasing online information overload problem and improve customer relationship management. Various recommender system techniques have been proposed since the mid-1990s, and many sorts of recommender system software have been developed recently for a variety of applications. Researchers and managers recognize that recommender systems offer great opportunities and challenges for business, government, education, and other domains, with more recent successful developments of recommender systems for real-world applications becoming apparent. It is thus vital that a high quality, instructive review of current trends should be conducted, not only of the theoretical research results but more importantly of the practical developments in recommender systems. This paper therefore reviews up-to-date application developments of recommender systems, clusters their applications into eight main categories: e-government, e-business, e-commerce/e-shopping, e-library, e-learning, e-tourism, e-resource services and e-group activities, and summarizes the related recommendation techniques used in each category. It systematically examines the reported recommender systems through four dimensions: recommendation methods (such as CF), recommender systems software (such as BizSeeker), real-world application domains (such as e-business) and application platforms (such as mobile-based platforms). Some significant new topics are identified and listed as new directions. By providing a state-of-the-art knowledge, this survey will directly support researchers and practical professionals in their understanding of developments in recommender system applications.},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Lu et al - 2015 - Recommender system application developments - A survey.pdf},
  journal    = {Decision Support Systems},
  keywords   = {E-commerce,E-government,E-learning,E-service personalization,Recommender systems},
  language   = {en}
}

@article{mhamdiJobRecommendationBased2020,
  title    = {Job {{Recommendation}} Based on {{Job Profile Clustering}} and {{Job Seeker Behavior}}},
  author   = {Mhamdi, D. and Moulouki, R. and El Ghoumari, M.Y. and Azzouazi, M. and Moussaid, L.},
  year     = {2020},
  volume   = {175},
  pages    = {695--699},
  issn     = {18770509},
  doi      = {10.1016/j.procs.2020.07.102},
  file     = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Mhamdi et al - 2020 - Job Recommendation based on Job Profile Clustering and Job Seeker Behavior.pdf},
  journal  = {Procedia Computer Science},
  language = {en}
}

@inproceedings{pochRankingJobOffers2014,
  title      = {Ranking Job Offers for Candidates: Learning Hidden Knowledge from Big Data},
  shorttitle = {Ranking Job Offers for Candidates},
  booktitle  = {In: {{Calzolari N}}, {{Choukri K}}, {{Declerck T}}, {{Loftsson H}}, {{Maegaard B}}, {{Mariani J}}, {{Moreno A}}, {{Odijk J}}, {{Piperidis S}}, Editors. {{Proceedings}} of the {{Ninth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}-2014); 2014 {{May}} 26-31; {{Reykjavik}}, {{Iceland}}. {{Paris}}: {{European Language Resources Association}}; 2014. p. 2076-82.},
  author     = {Poch, Marc and Bel Rafecas, N{\'u}ria and Espeja, Sergio and Navio, Felipe},
  year       = {2014},
  publisher  = {{ACL (Association for Computational Linguistics)}},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Poch et al - 2014 - Ranking job offers for candidates - learning hidden knowledge from big data.pdf}
}

@article{qiangShortTextTopic2020,
  title      = {Short {{Text Topic Modeling Techniques}}, {{Applications}}, and {{Performance}}: {{A Survey}}},
  shorttitle = {Short {{Text Topic Modeling Techniques}}, {{Applications}}, and {{Performance}}},
  author     = {Qiang, Jipeng and Qian, Zhenyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
  year       = {2020},
  pages      = {1--1},
  issn       = {1041-4347, 1558-2191, 2326-3865},
  doi        = {10.1109/TKDE.2020.2992485},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Qiang et al - 2020 - Short Text Topic Modeling Techniques, Applications, and Performance - A Survey.pdf},
  journal    = {IEEE Transactions on Knowledge and Data Engineering}
}

@article{qinResearchProgressSemiSupervised2019,
  title    = {Research {{Progress}} on {{Semi}}-{{Supervised Clustering}}},
  author   = {Qin, Yue and Ding, Shifei and Wang, Lijuan and Wang, Yanru},
  year     = {2019},
  month    = oct,
  volume   = {11},
  pages    = {599--612},
  issn     = {1866-9964},
  doi      = {10.1007/s12559-019-09664-w},
  abstract = {Semi-supervised clustering is a new learning method which combines semi-supervised learning (SSL) and cluster analysis. It is widely valued and applied to machine learning. Traditional unsupervised clustering algorithm based on data partition does not need any property; however, there are a small amount of independent class labels or pair constraint information data samples in practice; in order to obtain better clustering results, scholars have proposed a semi-supervised clustering. Compared with traditional clustering methods, it can effectively improve clustering performance through a small number of supervised information, and it has been used widely in machine learning. Firstly, this paper introduces the research status and classification of semi-supervised learning and compares the four classification methods as follows: decentralized model, support vector machine, graph, and collaborative training. Secondly, the semi-supervised clustering is described in detail, the current status of semi-supervised clustering is analyzed, and the Cop-kmeans algorithm, Lcop-kmeans algorithm, Seeded-kmeans algorithm, SC-kmeans algorithm, and other algorithms are introduced. The introduction of several semi-supervised clustering methods in this paper can show the advantages of semi-supervised clustering over traditional clustering, and the related literature in recent years is summarized. This paper summarized the latest development of semi-supervised learning and semi-supervised clustering and discussed the application of semi-supervised clustering and the future research direction.},
  file     = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Qin et al - 2019 - Research Progress on Semi-Supervised Clustering.pdf},
  journal  = {Cognitive Computation},
  language = {en},
  number   = {5}
}

@inproceedings{raunakEffectiveDimensionalityReduction2019,
  title     = {Effective {{Dimensionality Reduction}} for {{Word Embeddings}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Representation Learning}} for {{NLP}} ({{RepL4NLP}}-2019)},
  author    = {Raunak, Vikas and Gupta, Vivek and Metze, Florian},
  year      = {2019},
  month     = aug,
  pages     = {235--243},
  publisher = {{Association for Computational Linguistics}},
  address   = {{Florence, Italy}},
  doi       = {10.18653/v1/W19-4328},
  abstract  = {Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on improving the pretrained word vectors through post-processing algorithms. One improvement area is reducing the dimensionality of word embeddings. Reducing the size of word embeddings can improve their utility in memory constrained devices, benefiting several real world applications. In this work, we present a novel technique that efficiently combines PCA based dimensionality reduction with a recently proposed post-processing algorithm (Mu and Viswanath, 2018), to construct effective word embeddings of lower dimensions. Empirical evaluations on several benchmarks show that our algorithm efficiently reduces the embedding size while achieving similar or (more often) better performance than original embeddings. We have released the source code along with this paper.},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Raunak et al - 2019 - Effective Dimensionality Reduction for Word Embeddings.pdf}
}

@article{reddySemisupervisedLearningBrief2018,
  title      = {Semi-Supervised Learning: A Brief Review},
  shorttitle = {Semi-Supervised Learning},
  author     = {Reddy, Y. C. A. Padmanabha and Viswanath, P. and Reddy, B. Eswara},
  year       = {2018},
  month      = feb,
  volume     = {7},
  pages      = {81--85},
  issn       = {2227-524X},
  doi        = {10.14419/ijet.v7i1.8.9977},
  abstract   = {Most of the application domain suffers from not having sufficient labeled data whereas unlabeled data is available cheaply. To get labeled instances, it is very difficult because experienced domain experts are required to label the unlabeled data patterns. Semi-supervised learning addresses this problem and act as a half way between supervised and unsupervised learning. This paper addresses few techniques of Semi-supervised learning (SSL) such as self-training, co-training, multi-view learning, TSVMs methods. Traditionally SSL is classified in to Semi-supervised Classification and Semi-supervised Clustering which achieves better accuracy than traditional supervised and unsupervised learning techniques. The paper also addresses the issue of scalability and applications of Semi-supervised learning.},
  copyright  = {Copyright (c) 2018 International Journal of Engineering \& Technology},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Reddy et al - 2018 - Semi-supervised learning - a brief review.pdf},
  journal    = {International Journal of Engineering \& Technology},
  keywords   = {Labeled Data,Semi-Supervised Learning,SSL Methods,Test Data.,Training Data,Unlabeled Data},
  language   = {en-US},
  number     = {1.8}
}

@article{shiUnsupervisedLearningRandom2006,
  title     = {Unsupervised Learning with Random Forest Predictors},
  author    = {Shi, Tao and Horvath, Steve},
  year      = {2006},
  volume    = {15},
  pages     = {118--138},
  publisher = {{Taylor \& Francis}},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Shi_Horvath - 2006 - Unsupervised learning with random forest predictors.pdf},
  journal   = {Journal of Computational and Graphical Statistics},
  number    = {1}
}

@article{smithSearchingWorkDigital2015,
  title   = {Searching for Work in the Digital Era},
  author  = {Smith, Aaron},
  year    = {2015},
  volume  = {19},
  file    = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB STATS\\Smith - 2015 - Searching for work in the digital era.pdf},
  journal = {Pew Research Center}
}

@inproceedings{sunTextualDocumentClustering2014,
  title     = {Textual {{Document Clustering Using Topic Models}}},
  booktitle = {2014 10th {{International Conference}} on {{Semantics}}, {{Knowledge}} and {{Grids}}},
  author    = {Sun, Xiaoping},
  year      = {2014},
  month     = aug,
  pages     = {1--4},
  publisher = {{IEEE}},
  address   = {{Beijing, China}},
  doi       = {10.1109/SKG.2014.27},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Sun - 2014 - Textual Document Clustering Using Topic Models.pdf},
  isbn      = {978-1-4799-6715-5}
}

@article{suyalTextClusteringAlgorithms2014,
  title      = {Text {{Clustering Algorithms}}: {{A Review}}},
  shorttitle = {Text {{Clustering Algorithms}}},
  author     = {Suyal, Himanshu and Panwar, Amit and Singh Negi, Ajit},
  year       = {2014},
  month      = jun,
  volume     = {96},
  pages      = {36--40},
  issn       = {09758887},
  doi        = {10.5120/16946-7075},
  file       = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\CLUSTERING THEORY\\Suyal et al - 2014 - Text Clustering Algorithms - A Review.pdf},
  journal    = {International Journal of Computer Applications},
  number     = {24}
}

@inproceedings{vinelExperimentalComparisonUnsupervised2019,
  title     = {Experimental Comparison of Unsupervised Approaches in the Task of Separating Specializations within Professions in Job Vacancies},
  booktitle = {Conference on {{Artificial Intelligence}} and {{Natural Language}}},
  author    = {Vinel, Mikhail and Ryazanov, Ivan and Botov, Dmitriy and Nikolaev, Ivan},
  year      = {2019},
  pages     = {99--112},
  publisher = {{Springer}},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Vinel et al - 2019 - Experimental comparison of unsupervised approaches in the task of separating.pdf}
}

@inproceedings{worldeconomicforumFutureJobsReport2020,
  title     = {The {{Future}} of {{Jobs Report}} 2020},
  author    = {{World Economic Forum}},
  year      = {2020},
  publisher = {{World Economic Forum, Geneva, Switzerland}},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB STATS\\World Economic Forum - 2020 - The Future of Jobs Report 2020.pdf}
}

@inproceedings{yinDirichletMultinomialMixture2014,
  title     = {A Dirichlet Multinomial Mixture Model-Based Approach for Short Text Clustering},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author    = {Yin, Jianhua and Wang, Jianyong},
  year      = {2014},
  month     = aug,
  pages     = {233--242},
  publisher = {{ACM}},
  address   = {{New York New York USA}},
  doi       = {10.1145/2623330.2623715},
  abstract  = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.},
  file      = {E\:\\THIENDHB_GOOGLEDRIVE\\MASTER TILBURG\\THESIS\\REFERENCES\\JOB CLUSTERING\\Yin_Wang - 2014 - A dirichlet multinomial mixture model-based approach for short text clustering.pdf},
  isbn      = {978-1-4503-2956-9},
  language  = {en}
}


