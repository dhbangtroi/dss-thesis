{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to work cross-platform\n",
    "import os\n",
    "\n",
    "# Libraries to work with dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast                      # convert string to list after importing csv data\n",
    "import pickle\n",
    "\n",
    "# Libraries to cluster data\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Libraries to visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import matplotlib.cm as cm\n",
    "from cluster_visualization_helper import (\n",
    "    visualize_cluster)  # user-defined functions\n",
    "\n",
    "# Libraries for evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Libraries for monitoring operation process\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurate and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_name = os.name\n",
    "\n",
    "if os_name == 'nt':\n",
    "    \"\"\"Windows platform\"\"\"\n",
    "    BASE_DIR = \"E:/THIENDHB_GOOGLEDRIVE/MASTER TILBURG/THESIS/\"\n",
    "\n",
    "elif os_name == 'posix':\n",
    "    \"\"\"Linux platform\"\"\"\n",
    "    BASE_DIR = \"/media/pinkalinux/WORK/THIENDHB_GOOGLEDRIVE/MASTER TILBURG/THESIS/\"\n",
    "\n",
    "INPUT_DIR = BASE_DIR + \"DATASET/INPUT/\"\n",
    "OUTPUT_DIR = BASE_DIR + \"DATASET/OUTPUT/\"\n",
    "RESULT_DIR = BASE_DIR + \"RESULTS/\"\n",
    "\n",
    "SEED = 6886\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257205, 150)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_embeddings = np.load(OUTPUT_DIR + \"skill_feat_halfsize_embeddings.npy\")\n",
    "skill_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257205, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_docs = pd.read_csv(\n",
    "    OUTPUT_DIR + \"skill_tokens_long_df.csv\",\n",
    "    converters={\n",
    "        \"skill_token\": ast.literal_eval,\n",
    "    },\n",
    "    dtype={\n",
    "        \"key_id\": int,\n",
    "        \"skill_id\": int,\n",
    "        \"job_id\": int,\n",
    "        \"column_type\": str,\n",
    "        \"skill\": str,\n",
    "        \"skill_lemma\": str\n",
    "    }\n",
    ")\n",
    "skill_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>key_id</th>\n",
       "      <th>job_id</th>\n",
       "      <th>skill_id</th>\n",
       "      <th>column_id</th>\n",
       "      <th>column_type</th>\n",
       "      <th>skill</th>\n",
       "      <th>skill_lemma</th>\n",
       "      <th>skill_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10101</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>job_description</td>\n",
       "      <td>ameria investment consulting company</td>\n",
       "      <td>ameria investment consult company</td>\n",
       "      <td>[ameria, investment, consult, company]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10201</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>job_description</td>\n",
       "      <td>requires high level</td>\n",
       "      <td>require high level</td>\n",
       "      <td>[require, high, level]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10301</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>job_description</td>\n",
       "      <td>provides highly responsible</td>\n",
       "      <td>provide highly responsible</td>\n",
       "      <td>[provide, highly, responsible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10401</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>job_description</td>\n",
       "      <td>complex staff assistance</td>\n",
       "      <td>complex staff assistance</td>\n",
       "      <td>[complex, staff, assistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10501</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>job_description</td>\n",
       "      <td>chief financial officer</td>\n",
       "      <td>chief financial officer</td>\n",
       "      <td>[chief, financial, officer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257200</th>\n",
       "      <td>190010103</td>\n",
       "      <td>1900101</td>\n",
       "      <td>19001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>job_qualification</td>\n",
       "      <td>ra financial system</td>\n",
       "      <td>ra financial system</td>\n",
       "      <td>[ra, financial, system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257201</th>\n",
       "      <td>190010203</td>\n",
       "      <td>1900102</td>\n",
       "      <td>19001</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>job_qualification</td>\n",
       "      <td>higher legal education</td>\n",
       "      <td>high legal education</td>\n",
       "      <td>[high, legal, education]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257202</th>\n",
       "      <td>190010303</td>\n",
       "      <td>1900103</td>\n",
       "      <td>19001</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>job_qualification</td>\n",
       "      <td>high pressure environment</td>\n",
       "      <td>high pressure environment</td>\n",
       "      <td>[high, pressure, environment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257203</th>\n",
       "      <td>190010403</td>\n",
       "      <td>1900104</td>\n",
       "      <td>19001</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>job_qualification</td>\n",
       "      <td>professional work experience</td>\n",
       "      <td>professional work experience</td>\n",
       "      <td>[professional, work, experience]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257204</th>\n",
       "      <td>190010503</td>\n",
       "      <td>1900105</td>\n",
       "      <td>19001</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>job_qualification</td>\n",
       "      <td>work experience</td>\n",
       "      <td>work experience</td>\n",
       "      <td>[work, experience]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257205 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           row_id   key_id  job_id  skill_id  column_id        column_type  \\\n",
       "0           10101      101       1         1          1    job_description   \n",
       "1           10201      102       1         2          1    job_description   \n",
       "2           10301      103       1         3          1    job_description   \n",
       "3           10401      104       1         4          1    job_description   \n",
       "4           10501      105       1         5          1    job_description   \n",
       "...           ...      ...     ...       ...        ...                ...   \n",
       "257200  190010103  1900101   19001         1          3  job_qualification   \n",
       "257201  190010203  1900102   19001         2          3  job_qualification   \n",
       "257202  190010303  1900103   19001         3          3  job_qualification   \n",
       "257203  190010403  1900104   19001         4          3  job_qualification   \n",
       "257204  190010503  1900105   19001         5          3  job_qualification   \n",
       "\n",
       "                                       skill  \\\n",
       "0       ameria investment consulting company   \n",
       "1                        requires high level   \n",
       "2                provides highly responsible   \n",
       "3                   complex staff assistance   \n",
       "4                    chief financial officer   \n",
       "...                                      ...   \n",
       "257200                   ra financial system   \n",
       "257201                higher legal education   \n",
       "257202             high pressure environment   \n",
       "257203          professional work experience   \n",
       "257204                       work experience   \n",
       "\n",
       "                              skill_lemma  \\\n",
       "0       ameria investment consult company   \n",
       "1                      require high level   \n",
       "2              provide highly responsible   \n",
       "3                complex staff assistance   \n",
       "4                 chief financial officer   \n",
       "...                                   ...   \n",
       "257200                ra financial system   \n",
       "257201               high legal education   \n",
       "257202          high pressure environment   \n",
       "257203       professional work experience   \n",
       "257204                    work experience   \n",
       "\n",
       "                                   skill_token  \n",
       "0       [ameria, investment, consult, company]  \n",
       "1                       [require, high, level]  \n",
       "2               [provide, highly, responsible]  \n",
       "3                 [complex, staff, assistance]  \n",
       "4                  [chief, financial, officer]  \n",
       "...                                        ...  \n",
       "257200                 [ra, financial, system]  \n",
       "257201                [high, legal, education]  \n",
       "257202           [high, pressure, environment]  \n",
       "257203        [professional, work, experience]  \n",
       "257204                      [work, experience]  \n",
       "\n",
       "[257205 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_clusterer(X, min_samples, min_cluster_size, memory):\n",
    "    \"\"\"Generate clusters using HDBSCAN method\n",
    "    Hierarchical Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "    Args:\n",
    "        X:                 Matrix of features\n",
    "                             (n_samples, n_features)\n",
    "        min_samples:       The number of samples in a neighborhood for a point\n",
    "                             to be considered as a core point\n",
    "                             (int, default=None)\n",
    "        min_cluster_size:  The minimum size of clusters\n",
    "                             (int, default=5)\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model based on X\n",
    "    \"\"\"\n",
    "    clusterer = HDBSCAN(\n",
    "        min_samples=min_samples,\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        algorithm='boruvka_kdtree',\n",
    "        gen_min_span_tree=True,\n",
    "        core_dist_n_jobs=8,\n",
    "        memory=memory,\n",
    "    )\n",
    "    clusterer.fit(X)\n",
    "    return clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define search space for tuning hyperparameters\n",
    "X = skill_embeddings\n",
    "mem = Memory(location=OUTPUT_DIR + \"/cachedir\")\n",
    "tuning_result = {\n",
    "    \"n_clusters\": [],\n",
    "    \"n_noises\": [],\n",
    "    \"min_samples\": [],\n",
    "    \"min_cluster_size\": [],\n",
    "    \"duration\": [],\n",
    "}\n",
    "model_list = []\n",
    "label_list = []\n",
    "outlier_list = []\n",
    "exemplar_list = []\n",
    "min_samples_list = [3, 5, 10, 25, 50]\n",
    "min_cluster_size_list = [2, 3, 4, 5, 10, 25, 50]\n",
    "len(min_samples_list) * len(min_cluster_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning HDBSCAN Clustering:   0%|                                                                | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loop 2021-05-20 01:12:08.814119\n",
      "Start 2021-05-20 01:12:08.853176\n",
      "min_samples = 3\n",
      "min_cluster_size = 2\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling hdbscan.hdbscan_._hdbscan_boruvka_kdtree...\n",
      "_hdbscan_boruvka_kdtree(array([[-8.807369e-08, ..., -4.767032e-02],\n",
      "       ...,\n",
      "       [-9.632630e-08, ..., -8.618547e-02]]), \n",
      "3, 1.0, 'euclidean', None, 40, True, True, 8)\n"
     ]
    }
   ],
   "source": [
    "# Tuning hyperparameter\n",
    "start_loop_time = datetime.now()\n",
    "print(\"Start loop\", start_loop_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "for min_samples in tqdm(\n",
    "    iterable=min_samples_list,\n",
    "    desc=\"Tuning HDBSCAN Clustering\",\n",
    "    total=len(min_samples_list) * len(min_cluster_size_list),\n",
    "):\n",
    "    for min_cluster_size in min_cluster_size_list:\n",
    "\n",
    "        # Train model\n",
    "        starttime = datetime.now()\n",
    "        print(\"Start\", starttime.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        print(\"min_samples =\", min_samples)\n",
    "        print(\"min_cluster_size =\", min_cluster_size)\n",
    "\n",
    "        clusterer = hdbscan_clusterer(X, min_samples, min_cluster_size, mem)\n",
    "\n",
    "        endtime = datetime.now()\n",
    "        print(\"End\", endtime.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        print(\"Duration\", endtime - starttime)\n",
    "\n",
    "        # Save model\n",
    "        pickle.dump(\n",
    "            clusterer,\n",
    "            open(\n",
    "                OUTPUT_DIR\n",
    "                + \"hdbscan/\"\n",
    "                + \"skill_hdbscan_model_min_samples-\"\n",
    "                + str(min_samples)\n",
    "                + \"_min_cluster_size-\"\n",
    "                + str(min_cluster_size)\n",
    "                + \".pkl\",\n",
    "                \"wb\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        model_list.append(clusterer)\n",
    "        labels = clusterer.labels_\n",
    "        label_list.append(labels)\n",
    "        outlier_list.append(clusterer.outlier_scores_)\n",
    "        exemplar_list.append(clusterer.exemplars_)\n",
    "        tuning_result[\"duration\"].append(round((endtime - starttime).seconds / 60, 4))\n",
    "        tuning_result[\"min_samples\"].append(min_samples)\n",
    "        tuning_result[\"min_cluster_size\"].append(min_cluster_size)\n",
    "        tuning_result[\"n_noises\"].append(np.sum(np.array(labels) == -1, axis=0))\n",
    "        tuning_result[\"n_clusters\"].append(np.sum(np.unique(labels) != -1, axis=0))\n",
    "\n",
    "#         tqdm_bar.update(1)\n",
    "\n",
    "# tqdm_bar.close()\n",
    "end_loop_time = datetime.now()\n",
    "print(\"End loop\", end_loop_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "print(\"Duration\", end_loop_time - start_loop_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_result[\"silhouette\"] = []\n",
    "tuning_result[\"silhouette_error\"] = []\n",
    "\n",
    "tqdm_bar = tqdm(desc=\"Computing Silhouette score\", total=len(model_list))\n",
    "for i, _ in enumerate(model_list):\n",
    "    try:\n",
    "        silhouette = metrics.silhouette_score(\n",
    "            X, label_list[i], sample_size=10000, random_state=SEED, n_jobs=-1\n",
    "        )\n",
    "        tuning_result[\"silhouette_error\"].append(\"None\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        silhouette = -1.1\n",
    "        tuning_result[\"silhouette_error\"].append(e)\n",
    "    tuning_result[\"silhouette\"].append(silhouette)\n",
    "    tqdm_bar.update(1)\n",
    "tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_result[\"calinski_harabasz\"] = []\n",
    "tuning_result[\"calinski_harabasz_error\"] = []\n",
    "\n",
    "tqdm_bar = tqdm(desc=\"Computing Calinski Harabasz score\", total=len(model_list))\n",
    "for i, _ in enumerate(model_list):\n",
    "    try:\n",
    "        calinski_harabasz = metrics.calinski_harabasz_score(X, label_list[i])\n",
    "        tuning_result[\"calinski_harabasz_error\"].append(\"None\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        calinski_harabasz = -1.1\n",
    "        tuning_result[\"calinski_harabasz_error\"].append(e)\n",
    "    tuning_result[\"calinski_harabasz\"].append(calinski_harabasz)\n",
    "    tqdm_bar.update(1)\n",
    "tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_result[\"davies_bouldin\"] = []\n",
    "tuning_result[\"davies_bouldin_error\"] = []\n",
    "\n",
    "tqdm_bar = tqdm(desc=\"Computing Davies Bouldin score\", total=len(model_list))\n",
    "for i, _ in enumerate(model_list):\n",
    "    try:\n",
    "        davies_bouldin = metrics.davies_bouldin_score(X, label_list[i])\n",
    "        tuning_result[\"davies_bouldin_error\"].append(\"None\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        davies_bouldin = -1.1\n",
    "        tuning_result[\"davies_bouldin_error\"].append(e)\n",
    "    tuning_result[\"davies_bouldin\"].append(davies_bouldin)\n",
    "    tqdm_bar.update(1)\n",
    "tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_result[\"silhouette_corr\"] = []\n",
    "tuning_result[\"silhouette_corr_error\"] = []\n",
    "\n",
    "tqdm_bar = tqdm(desc=\"Computing Silhouette (correlation) score\", total=len(model_list))\n",
    "for i, _ in enumerate(model_list):\n",
    "    try:\n",
    "        silhouette = metrics.silhouette_score(\n",
    "            X,\n",
    "            label_list[i],\n",
    "            sample_size=10000,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            metric=\"correlation\",\n",
    "        )\n",
    "        tuning_result[\"silhouette_corr_error\"].append(\"None\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        silhouette = -1.1\n",
    "        tuning_result[\"silhouette_corr_error\"].append(e)\n",
    "    tuning_result[\"silhouette_corr\"].append(silhouette)\n",
    "    tqdm_bar.update(1)\n",
    "tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tuning results\n",
    "tuning_result_df = pd.DataFrame(tuning_result)\n",
    "tuning_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuning results\n",
    "tuning_result_df.to_csv(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_tuning_result.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine model sets\n",
    "tuple_objects = (\n",
    "    model_list,\n",
    "    label_list,\n",
    "    outlier_list,\n",
    "    exemplar_list,\n",
    "    tuning_result,\n",
    ")\n",
    "len(tuple_objects), len(tuple_objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuple of model sets\n",
    "pickle.dump(\n",
    "    tuple_objects, open(RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_model_tuning_list.pkl\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate cluster performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate other validation indices\n",
    "model_validation = {\n",
    "    \"min_samples\": tuning_result[\"min_samples\"],\n",
    "    \"min_cluster_size\": tuning_result[\"min_cluster_size\"],\n",
    "    \"n_clusters\": tuning_result[\"n_clusters\"],\n",
    "    \"relative_validity\": [],\n",
    "    \"cluster_persistence\": [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(model_list))):\n",
    "    model = model_list[i]\n",
    "\n",
    "    # Retrieve relative validity\n",
    "    \"\"\"\n",
    "    float\n",
    "    A fast approximation of the Density Based Cluster Validity (DBCV) score. \n",
    "    The only difference, and the speed, comes from the fact that \n",
    "    this relative_validity_ is computed using \n",
    "    the mutual- reachability minimum spanning tree, i.e. minimum_spanning_tree_, \n",
    "    instead of the all-points minimum spanning tree used in the reference. \n",
    "    This score might not be an objective measure of \n",
    "    the goodness of clusterering. \n",
    "    It may only be used to compare results across \n",
    "    different choices of hyper-parameters, therefore is only a relative score.\n",
    "    \"\"\"\n",
    "    relative_validity = model.relative_validity_\n",
    "\n",
    "    # Retrieve cluster persistence\n",
    "    \"\"\"\n",
    "    ndarray, shape (n_clusters, )\n",
    "    A score of how persistent each cluster is. \n",
    "    A score of 1.0 represents a perfectly stable cluster \n",
    "    that persists over all distance scales, \n",
    "    while a score of 0.0 represents a perfectly ephemeral cluster. \n",
    "    These scores can be gauge the relative coherence \n",
    "    of the clusters output by the algorithm.\n",
    "    \"\"\"\n",
    "    cluster_persistence = model.cluster_persistence_\n",
    "\n",
    "    model_validation[\"relative_validity\"].append(relative_validity)\n",
    "    model_validation[\"cluster_persistence\"].append(cluster_persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validation_df = pd.DataFrame(\n",
    "    {\n",
    "        \"min_samples\": model_validation[\"min_samples\"],\n",
    "        \"min_cluster_size\": model_validation[\"min_cluster_size\"],\n",
    "        \"n_clusters\": model_validation[\"n_clusters\"],\n",
    "        \"relative_validity\": model_validation[\"relative_validity\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation results\n",
    "model_validation_df.to_csv(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_validation_result.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine model sets\n",
    "tuple_objects2 = (\n",
    "    model_list,\n",
    "    label_list,\n",
    "    outlier_list,\n",
    "    exemplar_list,\n",
    "    tuning_result,\n",
    "    model_validation,\n",
    ")\n",
    "len(tuple_objects2), len(tuple_objects2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuple\n",
    "pickle.dump(\n",
    "    tuple_objects2,\n",
    "    open(OUTPUT_DIR + \"hdbscan/\" + \"skill_hdbscan_tuning_validation_list.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of markers for subplots\n",
    "marker_dict = {\n",
    "    \".\": \"point\",\n",
    "    \",\": \"pixel\",\n",
    "    \"o\": \"circle\",\n",
    "    \"v\": \"triangle_down\",\n",
    "    \"^\": \"triangle_up\",\n",
    "    \"<\": \"triangle_left\",\n",
    "    \">\": \"triangle_right\",\n",
    "    \"1\": \"tri_down\",\n",
    "    \"2\": \"tri_up\",\n",
    "    \"3\": \"tri_left\",\n",
    "    \"4\": \"tri_right\",\n",
    "    \"8\": \"octagon\",\n",
    "    \"s\": \"square\",\n",
    "    \"p\": \"pentagon\",\n",
    "    \"*\": \"star\",\n",
    "    \"h\": \"hexagon1\",\n",
    "    \"H\": \"hexagon2\",\n",
    "    \"+\": \"plus\",\n",
    "    \"x\": \"x\",\n",
    "    \"D\": \"diamond\",\n",
    "    \"d\": \"thin_diamond\",\n",
    "    \"|\": \"vline\",\n",
    "    \"_\": \"hline\",\n",
    "    \"P\": \"plus_filled\",\n",
    "    \"X\": \"x_filled\",\n",
    "    0: \"tickleft\",\n",
    "    1: \"tickright\",\n",
    "    2: \"tickup\",\n",
    "    3: \"tickdown\",\n",
    "    4: \"caretleft\",\n",
    "    5: \"caretright\",\n",
    "    6: \"caretup\",\n",
    "    7: \"caretdown\",\n",
    "    8: \"caretleftbase\",\n",
    "    9: \"caretrightbase\",\n",
    "    10: \"caretupbase\",\n",
    "    11: \"caretdownbase\",\n",
    "}\n",
    "marker_list = list(marker_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"hls\", n_colors=len(min_cluster_size_list))\n",
    "\n",
    "# The higher the better\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for idx, min_cluster_size in enumerate(min_cluster_size_list):\n",
    "    plt.plot(\n",
    "        tuning_result_df[\"min_samples\"].loc[\n",
    "            tuning_result_df[\"min_cluster_size\"] == min_cluster_size\n",
    "        ],        \n",
    "        tuning_result_df[\"n_clusters\"].loc[\n",
    "            tuning_result_df[\"min_cluster_size\"] == min_cluster_size\n",
    "        ],        \n",
    "        label=\"Min cluster size = \"\n",
    "        + str(min_cluster_size),\n",
    "        color=palette[idx],\n",
    "        marker=marker_list[idx]\n",
    "    )\n",
    "plt.ylabel(\"Number of clusters\")\n",
    "# plt.yticks(np.unique(model_result_df[\"number_clusters\"]))\n",
    "plt.xlabel(\"Min samples\")\n",
    "# plt.xticks(np.unique(model_result_df[\"min_samples\"]))\n",
    "plt.title(\"Number of clusters for HDBSCAN clustering\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "\n",
    "# Saving plot as image\n",
    "fig.savefig(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_n_clusters_plot.png\",\n",
    "#     bbox_inches=\"tight\",\n",
    "#     dpi=150,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhoulette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"hls\", n_colors=len(min_samples_list))\n",
    "\n",
    "# The higher the better\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "for idx, min_samples in enumerate(min_samples_list):\n",
    "    ax1.plot(\n",
    "        tuning_result_df[\"min_cluster_size\"].loc[\n",
    "            tuning_result_df[\"min_samples\"] == min_samples\n",
    "        ],\n",
    "        tuning_result_df[\"silhouette\"].loc[\n",
    "            tuning_result_df[\"min_samples\"] == min_samples\n",
    "        ],\n",
    "        label=\"Min samples = \"\n",
    "        + str(min_samples), color=palette[idx],\n",
    "        marker=marker_list[idx]\n",
    "    )\n",
    "ax1.set_xlabel(\"Min Cluster Size\")\n",
    "# ax1.set_xticks(min_cluster_size_list)\n",
    "ax1.set_ylabel(\"Silhouette Score\")\n",
    "ax1.set_title(\"Silhouette score for HDBSCAN clustering\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.grid()\n",
    "\n",
    "# Saving plot as image\n",
    "fig.savefig(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_silhouette_plot.png\",\n",
    "#     bbox_inches=\"tight\",\n",
    "#     dpi=150,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski Harabasz Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"hls\", n_colors=len(min_samples_list))\n",
    "\n",
    "# The higher the better\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "for idx, min_samples in enumerate(min_samples_list):\n",
    "    ax1.plot(\n",
    "        tuning_result_df[\"min_cluster_size\"].loc[\n",
    "            tuning_result_df[\"min_samples\"] == min_samples\n",
    "        ],\n",
    "        tuning_result_df[\"calinski_harabasz\"].loc[\n",
    "            tuning_result_df[\"min_samples\"] == min_samples\n",
    "        ],\n",
    "        label=\"Min samples = \"\n",
    "        + str(min_samples), color=palette[idx],\n",
    "        marker=marker_list[idx]\n",
    "    )\n",
    "ax1.set_xlabel(\"Min Cluster Size\")\n",
    "# ax1.set_xticks(min_cluster_size_list)\n",
    "ax1.set_ylabel(\"Calinski Harabasz Score\")\n",
    "ax1.set_title(\"Calinski Harabasz score for HDBSCAN clustering\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.grid()\n",
    "\n",
    "# Saving plot as image\n",
    "fig.savefig(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_calinski_harabasz_plot.png\",\n",
    "#     bbox_inches=\"tight\",\n",
    "#     dpi=150,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"hls\", n_colors=len(min_samples_list))\n",
    "\n",
    "# Closer to 0 is better\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "for idx, min_samples in enumerate(min_samples_list):\n",
    "    ax1.plot(\n",
    "        tuning_result_df[\"min_cluster_size\"].loc[\n",
    "            tuning_result_df[\"min_samples\"] == min_samples\n",
    "        ],\n",
    "        tuning_result_df[\"davies_bouldin\"].loc[\n",
    "            tuning_result_df[\"min_samples\"] == min_samples\n",
    "        ],\n",
    "        label=\"Min samples = \"\n",
    "        + str(min_samples), color=palette[idx],\n",
    "        marker=marker_list[idx]\n",
    "    )\n",
    "ax1.set_xlabel(\"Min Cluster Size\")\n",
    "# ax1.set_xticks(min_cluster_size_list)\n",
    "ax1.set_ylabel(\"Davies-Bouldin Score\")\n",
    "ax1.set_title(\"Davies-Bouldin score for HDBSCAN clustering\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.grid()\n",
    "\n",
    "# Saving plot as image\n",
    "fig.savefig(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_davies_bouldin_plot.png\",\n",
    "#     bbox_inches=\"tight\",\n",
    "#     dpi=150,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"hls\", n_colors=len(min_cluster_size_list))\n",
    "\n",
    "# The higher the better\n",
    "plt.figure(figsize=(12, 6))\n",
    "for idx, min_cluster_size in enumerate(min_cluster_size_list):\n",
    "    plt.plot(\n",
    "        model_validation_df[\"min_samples\"].loc[\n",
    "            model_validation_df[\"min_cluster_size\"] == min_cluster_size\n",
    "        ],\n",
    "        model_validation_df[\"relative_validity\"].loc[\n",
    "            model_validation_df[\"min_cluster_size\"] == min_cluster_size\n",
    "        ],\n",
    "        label=\"Min cluster size = \" + str(min_cluster_size),\n",
    "        color=palette[idx],\n",
    "        marker=marker_list[idx]\n",
    "    )\n",
    "plt.ylabel(\"Relative validity\")\n",
    "plt.xlabel(\"Min samples\")\n",
    "plt.title(\"Relative validity for HDBSCAN clustering\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "\n",
    "# Saving plot as image\n",
    "fig.savefig(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_relative-validity_plot.png\",\n",
    "#     bbox_inches=\"tight\",\n",
    "#     dpi=150,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_index = np.argmax(model_validation['relative_validity'])\n",
    "best_index = int(\n",
    "    np.where(\n",
    "        (np.array(tuning_result[\"min_samples\"]) == 5)\n",
    "        & (np.array(tuning_result[\"min_cluster_size\"]) == 50)\n",
    "    )[0]\n",
    ")\n",
    "print(best_index)\n",
    "\n",
    "best_min_samples = tuning_result_df[\"min_samples\"][best_index]\n",
    "best_min_cluster_size = tuning_result_df[\"min_cluster_size\"][best_index]\n",
    "print(best_min_samples, best_min_cluster_size)\n",
    "best_labels = label_list[best_index]\n",
    "best_n_clusters = tuning_result_df[\"n_clusters\"][best_index]\n",
    "best_n_noises = tuning_result_df[\"n_noises\"][best_index]\n",
    "print(best_n_clusters, best_n_noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_datapoint = np.load(OUTPUT_DIR + \"visualization/\" + \"skill_halfsize_pca_datapoints.npy\")\n",
    "umap_datapoint = np.load(OUTPUT_DIR + \"visualization/\" + \"skill_halfsize_umap_datapoints.npy\")\n",
    "tsne_datapoint = np.load(OUTPUT_DIR + \"visualization/\" + \"skill_halfsize_tsne_datapoints.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_title = (\n",
    "    \"HDBSCAN cluster visualization on skill embeddings \\n Number of clusters = \"\n",
    "    + str(best_n_clusters) + \" - Number of noises = \" + str(best_n_noises)\n",
    "    + \"\\n Min samples = \"\n",
    "    + str(best_min_samples)\n",
    "    + \" - Min cluster size = \"\n",
    "    + str(best_min_cluster_size)\n",
    ")\n",
    "plot_filename = (\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_optimal-cluster-visualization.png\"\n",
    ")\n",
    "palette = sns.color_palette(\"nipy_spectral\", as_cmap=True)\n",
    "# colors = cm.nipy_spectral(best_labels.astype(float) / best_k)\n",
    "colors = best_labels\n",
    "\n",
    "# Visualize clusters with PCA, UMAP, and t-SNE\n",
    "visualize_cluster(\n",
    "    plot_title,\n",
    "    (12, 15),\n",
    "    colors,\n",
    "    palette,\n",
    "    4,\n",
    "    pca_datapoint,\n",
    "    tsne_datapoint,\n",
    "    umap_datapoint,\n",
    "    pca=True,\n",
    "    tsne=True,\n",
    "    compute_umap=True,\n",
    "    save_plots=True,\n",
    "    plot_file=plot_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display top words of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = skill_docs.copy(deep=True)\n",
    "df_clusters[\"model_type\"] = \"genie\"\n",
    "df_clusters[\"skill_n_clusters\"] = best_n_clusters\n",
    "df_clusters['skill_cluster_label'] = best_labels\n",
    "df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount = {}\n",
    "sorted_wordcount = {}\n",
    "for i in range(best_n_clusters):\n",
    "    skills = df_clusters[df_clusters[\"skill_cluster_label\"] == i][\"skill\"].values\n",
    "    skills = \" \".join(\" \".join(skills).split())\n",
    "    wordcount[i] = {}\n",
    "    for j in skills.split():\n",
    "        if j in wordcount[i]:\n",
    "            wordcount[i][j] += 1\n",
    "        else:\n",
    "            wordcount[i][j] = 1\n",
    "    sorted_wordcount[i] = sorted(wordcount[i].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = df_clusters.copy(deep=True)\n",
    "# tmp = tmp.loc[tmp['skill_cluster_label'] == 24]\n",
    "# tmp.iloc[50:100, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = {}\n",
    "for key, i in sorted_wordcount.items():\n",
    "    print(\"Cluster \" + str(key) + \": \", end=\"\")\n",
    "    topwords[key] = \"\"\n",
    "    n = 0\n",
    "    for newkey, j in sorted_wordcount[key][:10]:\n",
    "        print(newkey + \"|\", end=\"\")\n",
    "        topwords[key] = topwords[key] + newkey + \"|\"\n",
    "        if n == 10:\n",
    "            print(\"\\n------------ \", end=\"\")\n",
    "        n += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords2 = {}\n",
    "for key, i in sorted_wordcount.items():\n",
    "    print(\"Cluster \"+str(key)+\": \", end='')\n",
    "    topwords2[key] = ''\n",
    "    for newkey, j in sorted_wordcount[key][10:20]:\n",
    "        print(newkey + '|', end='')\n",
    "        topwords2[key] = topwords2[key] + newkey + '|'\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords3 = {}\n",
    "for key, i in sorted_wordcount.items():\n",
    "    print(\"Cluster \"+str(key)+\": \", end='')\n",
    "    topwords3[key] = ''\n",
    "    for newkey, j in sorted_wordcount[key][20:30]:\n",
    "        print(newkey + '|', end='')\n",
    "        topwords3[key] = topwords3[key] + newkey + '|'\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cluster results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters.to_csv(\n",
    "    RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_optimal-cluster-labels.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuple of model sets\n",
    "pickle.dump(\n",
    "    (sorted_wordcount, topwords, topwords2, topwords3),\n",
    "    open(RESULT_DIR + \"hdbscan/\" + \"skill_hdbscan_optimal-topwords.pkl\", \"wb\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
